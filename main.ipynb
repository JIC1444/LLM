{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using (B, ) matrix shape convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTransformation:\n",
    "    \"\"\"\n",
    "    A learned linear transformation y = x * W^T + b\n",
    "\n",
    "    in_size is the length of the vector input x.\n",
    "    out_size is the length of the vector output y.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size: int, out_size: int):\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        # Cached variables.\n",
    "        self.x = None\n",
    "        self.W = np.random.randn(self.in_size, self.out_size)\n",
    "        self.b = np.random.randn(self.out_size)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Ensure x is flat:\n",
    "        assert len(x.shape) <=2, f\"x must have shape (B, dim1),\\\n",
    "                        not {x.shape}\"\n",
    "        self.x = x\n",
    "        self.y = np.matmul(self.x, self.W) + self.b\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        # Find ∂L/∂W, ∂L/∂b and ∂L/∂x\n",
    "        # Find derivative w.r.t W.\n",
    "        self.dW = np.matmul(self.x.T, d_out)\n",
    "        self.db = np.sum(d_out, axis=0)\n",
    "        dx = np.matmul(d_out, self.W.T)\n",
    "        return dx\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.W = self.W - self.dW * lr\n",
    "        self.b = self.b - self.db * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test: LinearTransformation.linear_transformation(x).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): #https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / (e_x.sum(axis=0) + 1e-5) # For numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, embed_dim:int, num_heads=8):\n",
    "        assert embed_dim % num_heads == 0, \\\n",
    "            \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        # 8 heads in 'Attention is all you need'.\n",
    "        self.num_heads = num_heads \n",
    "\n",
    "        # Linear transformations.\n",
    "        self.linearQ = LinearTransformation(in_size=self.embed_dim,\n",
    "                                            out_size=self.embed_dim)\n",
    "        self.linearK = LinearTransformation(in_size=self.embed_dim,\n",
    "                                            out_size=self.embed_dim)\n",
    "        self.linearV = LinearTransformation(in_size=self.embed_dim,\n",
    "                                            out_size=self.embed_dim)\n",
    "        self.linearS = LinearTransformation(in_size=embed_dim \\\n",
    "                                            * num_heads,\n",
    "                                            out_size=embed_dim)\n",
    "        \n",
    "        # Cache for backpropagation.\n",
    "        self.Q = None\n",
    "        self.K = None\n",
    "        self.V = None\n",
    "        self.Qh = None\n",
    "        self.Kh = None\n",
    "        self.Vh = None\n",
    "        self.scores = None\n",
    "        self.attention_weights = None\n",
    "        self.context_per_head = None\n",
    "        self.context_merge = None\n",
    "        self.output = None\n",
    "\n",
    "        # Weights.\n",
    "        self.dQ = None\n",
    "        self.dK = None\n",
    "        self.dV = None\n",
    "\n",
    "\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # Step 1: Calculate LTs for Q, K, V.\n",
    "        self.Q = self.linearQ(x)\n",
    "        self.K = self.linearK(x)\n",
    "        self.V = self.linearV(x)\n",
    "\n",
    "        # Step 2: break dim -1 into dim -2 and -1 by \n",
    "        # dim -2 = dim -1//num_heads.\n",
    "        # i.e from (batch, dim-2, dim-1) into (batch, dim-3, dim-2, dim-1).\n",
    "        Q_reshape = self.Q.reshape(\n",
    "            self.Q.shape[0], \n",
    "            self.Q.shape[1],\n",
    "            self.num_heads,\n",
    "            self.Q.shape[2] // self.num_heads\n",
    "        )\n",
    "        K_reshape = self.K.reshape(\n",
    "            self.K.shape[0], \n",
    "            self.K.shape[1],\n",
    "            self.num_heads,\n",
    "            self.K.shape[2] // self.num_heads\n",
    "        )\n",
    "        V_reshape = self.V.reshape(\n",
    "            self.V.shape[0], \n",
    "            self.V.shape[1],\n",
    "            self.num_heads,\n",
    "            self.V.shape[2] // self.num_heads\n",
    "        )\n",
    "\n",
    "        # Step 3: permute the dimensions from:\n",
    "        # (batch, dim1, num_heads, dim3) to \n",
    "        # (batch, num_heads, dim1, dim3).\n",
    "        self.Qh = Q_reshape.transpose(0, 2, 1, 3)\n",
    "        self.Kh = K_reshape.transpose(0, 2, 1, 3)\n",
    "        self.Vh = V_reshape.transpose(0, 2, 1, 3) \n",
    "\n",
    "        # Step 4: Scaled Dot Product Attention.\n",
    "        # Step 4.1: permute K dims.\n",
    "        K_perm = np.swapaxes(self.Kh, -2, -1)\n",
    "\n",
    "        # Step 4.2: matrix multiply Q and K_transpose.\n",
    "        QK = np.matmul(self.Qh, K_perm)\n",
    "\n",
    "        # Step 4.3: scale prior product with the number of elements in K.\n",
    "        self.scores = QK / math.sqrt(self.Qh.shape[-1] * self.Qh.shape[-2])\n",
    "\n",
    "        # Step 4.4: mask (optional).\n",
    "\n",
    "        # Step 4.5: softmax.\n",
    "        self.attention_weights = softmax(self.scores, axis=-1)\n",
    "\n",
    "        # Step 4.6: matrix multiply prior product and V.\n",
    "        self.context_per_head = np.matmul(self.attention_weights, self.Vh)\n",
    "        \n",
    "        # Step 5: permute the num_heads back to dim 2.\n",
    "        context_transpose = self.context_per_head\\\n",
    "                                .transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Step 6: merge dimensions -1 and -2.\n",
    "        self.context_merge = context_transpose.reshape(\n",
    "            context_transpose.shape[0],\n",
    "            context_transpose.shape[1],\n",
    "            int(context_transpose.shape[2] *\\\n",
    "                     context_transpose.shape[3])\n",
    "        )\n",
    "        \n",
    "        # Step 7: linear transformation\n",
    "        self.output = self.linearS(self.context_merge)\n",
    "        return self.output\n",
    "        \n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Work out gradients ∂L/∂Q, ∂L/∂K, ∂L/∂V and dx.\n",
    "        B, T, E = self.x.shape\n",
    "\n",
    "        # Backprop through final linear projection.\n",
    "        d_concat = d_out @ self.W_o.T # (B, T, E)\n",
    "        self.dW_o = self.combine_heads(self.attn_output).reshape(B * T, E).T @ d_out.reshape(B * T, E)\n",
    "\n",
    "        # Split heads\n",
    "        d_attn_output = self.split_heads(d_concat)           # (B, H, T, D)\n",
    "\n",
    "        # d_attn_weights @ V => d_attn_weights, d_V_heads\n",
    "        d_attn_weights = d_attn_output @ self.V_heads.transpose(0,1,3,2)\n",
    "        d_V_heads = self.attn_weights.transpose(0,1,3,2) @ d_attn_output\n",
    "\n",
    "        # Softmax backward\n",
    "        # dS = softmax * (d - sum(softmax * d))\n",
    "        softmax = self.attn_weights\n",
    "        d_scores = d_attn_weights * softmax - softmax * np.sum(d_attn_weights * softmax, axis=-1, keepdims=True)\n",
    "\n",
    "        d_scores /= np.sqrt(self.head_dim)\n",
    "\n",
    "        # Q @ K^T => dQ_heads, dK_heads\n",
    "        d_Q_heads = d_scores @ self.K_heads\n",
    "        d_K_heads = d_scores.transpose(0,1,3,2) @ self.Q_heads\n",
    "\n",
    "        # Merge heads\n",
    "        d_Q = self.combine_heads(d_Q_heads)                  # (B, T, E)\n",
    "        d_K = self.combine_heads(d_K_heads)\n",
    "        d_V = self.combine_heads(d_V_heads)\n",
    "\n",
    "        # Backprop through input projections\n",
    "        self.dW_q = self.x.reshape(B*T, E).T @ d_Q.reshape(B*T, E)\n",
    "        self.dW_k = self.x.reshape(B*T, E).T @ d_K.reshape(B*T, E)\n",
    "        self.dW_v = self.x.reshape(B*T, E).T @ d_V.reshape(B*T, E)\n",
    "\n",
    "        # Chain rule to previous layer\n",
    "        dx_q = d_Q @ self.W_q.T\n",
    "        dx_k = d_K @ self.W_k.T\n",
    "        dx_v = d_V @ self.W_v.T\n",
    "        dx = dx_q + dx_k + dx_v\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update(self):\n",
    "        self.linearQ.update(lr=)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test: MultiHeadAttention.calculate_attention(Q, K, V).\n",
    "UTQ, UTK, UTV = np.random.rand(1, 8, 8), np.random.rand(1, 8, 8), np.random.rand(1, 8, 8)\n",
    "UT_calc_att = MultiHeadAttention(embed_dim=512)\n",
    "#UT_att = UT_calc_att.calculate_attention(UTQ, UTK, UTV)\n",
    "#print(UT_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab_length, embed_dim):\n",
    "        self.vocab_length = vocab_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.E = np.random.randn(vocab_length, embed_dim)\n",
    "\n",
    "    def positional_encoding(self, seq_length: int, embed_dim: int):\n",
    "        \"\"\"\n",
    "        Generates a positional encoding for a given length and depth.\n",
    "\n",
    "        Args:\n",
    "        - length: the length of the input sequence.\n",
    "        - embed_dim: the dimensionality of the encoding.\n",
    "\n",
    "        Returns:\n",
    "        - np.array, Positional encoding of shape (length, embed_dim).\n",
    "        \n",
    "        \"\"\"\n",
    "        embed_dim /= 2\n",
    "\n",
    "        positions = np.arange(seq_length)[:, np.newaxis]\n",
    "        depths = np.arange(embed_dim)[np.newaxis, :] / embed_dim\n",
    "\n",
    "        angle_rates = 1 / (10000**depths)\n",
    "        angle_rads = positions * angle_rates\n",
    "\n",
    "        pos_encoding = np.concatenate([\n",
    "                                    np.sin(angle_rads), \n",
    "                                    np.cos(angle_rads)], \n",
    "                                    axis=-1)\n",
    "        \n",
    "        return pos_encoding\n",
    "    \n",
    "    def embed_tokens(self, tokens, seq_length, embed_dim):\n",
    "        embed = self.E[tokens]\n",
    "        return embed + self.positional_encoding(seq_length, embed_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, embed_dim, eps=1e-5):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.eps = eps # Numerical stability.\n",
    "\n",
    "        # Initialize scale (gamma) and shift (beta).\n",
    "        self.gamma = np.ones((self.embed_dim, )) \n",
    "        self.beta = np.zeros((self.embed_dim, )) \n",
    "\n",
    "        # Initialize gradients.\n",
    "        self.dgamma = np.zeros_like(self.gamma)\n",
    "        self.dbeta = np.zeros_like(self.beta)\n",
    "\n",
    "        # Cache.\n",
    "        self.x = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "        self.std = None\n",
    "        self.x_norm = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.x = x      # Save for backpropagation.\n",
    "        self.mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        self.var = np.var(x, axis=-1, keepdims=True)\n",
    "        self.std = np.sqrt(self.var + self.eps)\n",
    "\n",
    "        self.x_norm = (x - self.mean) / self.std\n",
    "\n",
    "        return self.gamma[None, None, :] * self.x_norm \\\n",
    "                + self.beta[None, None, :]\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        # Compute ∂L/∂x, ∂L/∂gamma and ∂L/∂beta.\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    # From PyTorch documenation.\n",
    "    return 0.5 * x * (1 + np.tanh(\\\n",
    "        np.sqrt(2 / np.pi) \\\n",
    "        * x + 0.044715 * x ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2:\n",
    "    \"\"\"\n",
    "    Defines the overarching process, including tokenization.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len, batch, paper_dim=3072):\n",
    "        # Define all of the components which make up the transformer.\n",
    "        # Embedding for words.\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = Tokenizer(self.vocab_size, \n",
    "                                   self.embed_dim)\n",
    "        \n",
    "        self.max_seq_len = max_seq_len  # As each sample is padded.\n",
    "        self.batch = batch\n",
    "        self.dims = (self.batch, max_seq_len, self.embed_dim)\n",
    "\n",
    "        # Layer Norm.\n",
    "        self.layernorm1 = LayerNorm(embed_dim=self.embed_dim)\n",
    "        self.linear1 = LinearTransformation(in_size=self.embed_dim,\n",
    "                                            out_size=self.embed_dim)\n",
    "\n",
    "        # Multihead attention, Q, K, V are incorporated into this.\n",
    "        self.q_proj = LinearTransformation(in_size=self.embed_dim,\n",
    "                                           out_size=self.embed_dim)\n",
    "        self.k_proj = LinearTransformation(in_size=self.embed_dim,\n",
    "                                           out_size=self.embed_dim)\n",
    "        self.v_proj = LinearTransformation(in_size=self.embed_dim,\n",
    "                                           out_size=self.embed_dim)\n",
    "        self.multihead_attention = MultiHeadAttention\\\n",
    "                                    (embed_dim=self.embed_dim)\n",
    "        # Produces a matrix (Batch, max_seq_len, embed_dim).\n",
    "        \n",
    "        self.layernorm2 = LayerNorm(embed_dim=self.embed_dim)\n",
    "\n",
    "        # Feed forward is two linear transformation layers\n",
    "        # with gelu activation.\n",
    "        self.paper_dim = paper_dim # Richer context dimension <- 3072.\n",
    "        self.linearff1 = LinearTransformation(in_size=self.embed_dim,\n",
    "                                              out_size=self.paper_dim)\n",
    "        self.linearff2 = LinearTransformation(in_size=self.paper_dim,\n",
    "                                              out_size=self.embed_dim)\n",
    "\n",
    "        self.layernorm3 = LayerNorm(embed_dim=self.embed_dim)\n",
    "        self.linear3 = LinearTransformation(in_size=self.embed_dim,\n",
    "                                            out_size=self.embed_dim)\n",
    "\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Embed the entire input\n",
    "        # tokenized_input = self.tokenizer.\\\n",
    "        #                     embed_tokens(tokens= ,\n",
    "        #                                 seq_length= ,\n",
    "        #                                 embed_dim=)\n",
    "        \n",
    "        # Feed the tokens into block1 and save for residual connection.\n",
    "        tokenized_input = batch\n",
    "        ln1_out = self.layernorm1(tokenized_input)\n",
    "        lin1_out = self.linear1(ln1_out)\n",
    "\n",
    "        # Multihead attention.\n",
    "        Q = self.q_proj(lin1_out)\n",
    "        K = self.k_proj(lin1_out) \n",
    "        V = self.v_proj(lin1_out)\n",
    "        mha_out = self.multihead_attention(Q, K, V)\n",
    "\n",
    "        # Residual connection 1.\n",
    "        residual = mha_out + tokenized_input\n",
    "\n",
    "        # Layer Norm and FFN.\n",
    "        ln2_out = self.layernorm2(residual)\n",
    "        ff1_out = self.linearff1(ln2_out)\n",
    "        ff2_out = self.linearff2(ff1_out)\n",
    "        \n",
    "        # Concatenate the residual with ff_out.\n",
    "        concat = ff2_out + residual\n",
    "\n",
    "        # Final block and return softmax for probabilties.\n",
    "        ln3_out = self.layernorm3(concat)\n",
    "        lin3_out = self.linear3(ln3_out)\n",
    "        logits = softmax(lin3_out)\n",
    "\n",
    "        return logits\n",
    "            \n",
    "\n",
    "    def backward(self):\n",
    "        # Do the calculation for each loss for each layer, backwards.\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.91349995e-27 5.81111069e-13 5.73926770e-22 ... 1.19570838e-27\n",
      "   9.58005228e-15 2.66700377e-16]\n",
      "  [5.44747373e-40 3.21771446e-28 7.98375206e-30 ... 2.06267159e-17\n",
      "   1.62393800e-18 1.11615097e-24]\n",
      "  [1.31087789e-16 2.83409528e-23 3.67691288e-12 ... 4.87758839e-18\n",
      "   1.76014385e-37 9.28408266e-35]\n",
      "  ...\n",
      "  [5.30493913e-21 2.27255554e-26 7.29809773e-07 ... 1.41928656e-29\n",
      "   9.82121906e-27 1.20477375e-30]\n",
      "  [3.65983447e-27 7.59923740e-26 1.17513713e-19 ... 1.12300108e-19\n",
      "   1.98346432e-16 1.58426387e-35]\n",
      "  [3.72829596e-32 8.09315423e-08 1.74512267e-32 ... 1.80276927e-23\n",
      "   1.94020637e-25 1.18963248e-22]]\n",
      "\n",
      " [[6.17288596e-29 9.24793160e-16 2.39275596e-25 ... 8.33868400e-37\n",
      "   7.25870891e-24 1.11443872e-12]\n",
      "  [6.47713845e-14 2.26396379e-19 3.83992818e-23 ... 1.17170028e-19\n",
      "   1.07900847e-26 1.01314615e-18]\n",
      "  [2.87589966e-17 1.00618494e-28 3.21250249e-06 ... 6.69301236e-24\n",
      "   3.37230500e-29 2.17756533e-30]\n",
      "  ...\n",
      "  [3.00140612e-21 7.17045790e-23 3.87637550e-25 ... 7.09166978e-17\n",
      "   1.13065561e-19 2.96293919e-22]\n",
      "  [7.42241865e-14 7.93774657e-28 1.76890949e-23 ... 2.06970622e-26\n",
      "   2.19139391e-24 3.82321030e-30]\n",
      "  [2.36845555e-30 6.06807970e-26 2.02546422e-32 ... 7.42899417e-29\n",
      "   3.27466824e-38 1.58234681e-30]]\n",
      "\n",
      " [[1.92457216e-23 6.11287380e-13 4.81850599e-18 ... 2.66013454e-25\n",
      "   1.76404211e-33 1.09958560e-21]\n",
      "  [5.00989804e-20 3.54013891e-30 3.73311442e-26 ... 3.30559613e-32\n",
      "   6.99898143e-25 3.60818904e-21]\n",
      "  [1.13447076e-14 1.51892616e-26 4.39926954e-32 ... 6.81347973e-22\n",
      "   7.22290436e-14 4.79502932e-30]\n",
      "  ...\n",
      "  [3.12856457e-28 8.37652661e-10 1.27833214e-22 ... 2.74611151e-19\n",
      "   8.93548503e-27 3.09672233e-28]\n",
      "  [1.58601946e-29 2.53504250e-14 1.90988845e-30 ... 1.09786691e-17\n",
      "   3.09073184e-07 9.68285191e-17]\n",
      "  [1.70487291e-31 2.64930418e-23 2.74329935e-21 ... 3.43641861e-31\n",
      "   6.26651882e-28 6.58294867e-26]]\n",
      "\n",
      " [[4.18960586e-12 2.24180954e-29 8.12810569e-26 ... 4.16274518e-19\n",
      "   3.80976917e-22 4.97146305e-34]\n",
      "  [2.18515864e-34 2.78256054e-30 3.18200418e-29 ... 7.83019235e-19\n",
      "   1.54429487e-12 1.56921119e-11]\n",
      "  [1.09098789e-19 1.33225899e-27 1.39804421e-27 ... 1.27380140e-20\n",
      "   3.44097857e-26 2.26087428e-26]\n",
      "  ...\n",
      "  [1.68890249e-14 5.40925537e-25 1.99636066e-26 ... 3.47942431e-16\n",
      "   2.96444993e-29 1.58794022e-28]\n",
      "  [1.24462300e-24 7.48093242e-26 1.58518378e-13 ... 1.82586580e-27\n",
      "   1.41451707e-19 4.54976645e-25]\n",
      "  [5.25630948e-25 1.16425269e-25 1.03803766e-23 ... 9.54635994e-19\n",
      "   2.50368944e-15 2.36206771e-20]]]\n"
     ]
    }
   ],
   "source": [
    "# Unit test: GPT2.__call__(batch).\n",
    "# Random input.\n",
    "B, seq_len, embed_dim = 4, 10, 768\n",
    "x = np.random.randint(1, 768, (B, seq_len, embed_dim))\n",
    "model = GPT2(vocab_size=10000,\n",
    "             embed_dim=embed_dim,\n",
    "             max_seq_len=seq_len,\n",
    "             batch=B)\n",
    "output = model(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyTorch Loss function to save time.\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected argument value expression (3494379259.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[155]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mvocab_size= ,\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected argument value expression\n"
     ]
    }
   ],
   "source": [
    "model = GPT2(\n",
    "    vocab_size= ,\n",
    "    embed_dim= ,\n",
    "    max_seq_len= ,\n",
    "    batch= ,\n",
    ")\n",
    "\n",
    "def train(model, data, hparams):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
